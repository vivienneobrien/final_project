{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "go_back_lstm_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpLCBrS0DoTl",
        "colab_type": "text"
      },
      "source": [
        "# Notes for user:\n",
        "\n",
        "*   This notebook will use the dataset COVID-19 and forex to predict the forex spot price of a currency pair.\n",
        "*   What to ask user before using this application:\n",
        "*   What currency pair are you interested in? USD/GBP (This layout means that the base currency is USD)\n",
        "*   The currency the user wants to look at is how much the USD is worth to the GBP.\n",
        "*   Therefore, we look at the death rates in the US and the UK.\n",
        "*   LSTMs are good at looking at changed over time. We need to also consider that what happen more recently is a better judgement than what happened a long time ago. This is why CNNs might be interesting to explore.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDBxlQkCD7Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use some functions from tensorflow_docs\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ6D0PrOJYeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgIgQg1XJcP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_F-7tgZJe2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzadtzC3Hniw",
        "colab_type": "text"
      },
      "source": [
        "# Data Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GjIgZ7kJhjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing covid_dataset using pandas. Droping null values.\n",
        "covid_dataset_data= pd.read_csv('covid.csv', na_values = \"?\", comment='\\t', skipinitialspace=True)\n",
        "covid_dataset = covid_dataset_data.copy()\n",
        "# covid_dataset = covid_dataset.dropna()\n",
        "covid_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYBFYnKtiIkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trying to isolate location/country based on currency in question\n",
        "# We are looking at pound and sterling therefore need to isolate 'United Kingdom' & 'United States' in location column\n",
        "# Problem: Informaiton prints up until 'Trinidad and Tobago' for some reason â€“ can print 'Togo'. Therefore cannot print United Kingdom or United States\n",
        "us_death_rates = covid_dataset.loc[covid_dataset.location=='United States', ['location', 'date', 'total_deaths', 'total_cases', 'cvd_death_rate']]\n",
        "us_death_rates.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "285DiboRj7MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# us_death_rates = covid_dataset.loc[covid_dataset.location=='Zimbabwe', ['location', 'date', 'total_deaths','total_cases','cvd_death_rate']]\n",
        "# us_death_rates.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3wvK4d2kvti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "us_death_rates.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIDXEXFDf4N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import forex\n",
        "forex_dataset_data= pd.read_csv('GBP_USD.csv', na_values = \"?\", comment='\\t', skipinitialspace=True)\n",
        "forex_dataset = forex_dataset_data.copy()\n",
        "# forex_dataset = forex_dataset_data.dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C53hVpItH0az",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing / Data Cleaning & Collating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIpYbWz2HxyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter dataset\n",
        "forex_dataset.pop('Open')\n",
        "forex_dataset.pop('High')\n",
        "forex_dataset.pop('Low')\n",
        "forex_dataset.pop('Change %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSzzjHJxi5D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forex_dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrVetfeGgSm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Isolate covid_date\n",
        "import time \n",
        "us_date = us_death_rates['date']\n",
        "new_us_dates = [time.strptime(x.replace(\"-\",\" \"), '%Y %m %d')for x in us_date]\n",
        "us_death_rates['date'] = new_us_dates\n",
        "us_death_rates.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGR0p0YWh9jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Isolate forex_date\n",
        "import time \n",
        "forex_date = forex_dataset['Date']\n",
        "new_forex_dates = [time.strptime(x.replace(',', \"\"), '%b %d %Y')for x in forex_date]\n",
        "forex_dataset['Date'] = new_forex_dates\n",
        "forex_dataset.tail()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b16glGjiCLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(forex_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiFVDAF0ngrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merging dates from two tables\n",
        "dataset = pd.merge(left=forex_dataset, left_on='Date',\n",
        "         right=us_death_rates, right_on='date')\n",
        "dataset.tail()\n",
        "len(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9A_G1s9ETvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We pop the 'date' from the covid_dataset because this column starts later than the forex column\n",
        "# We want to see the forex price before covid happened\n",
        "dataset.pop('date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz0-4V24EY65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem: This still starts at 2019,12,13 when we want 2019,12,02\n",
        "# However this does not affect our data graph for some reason - scroll down\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKPMw6jUypAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Object means string\n",
        "dataset['total_deaths']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9kHcA_4tAqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View what date looks like\n",
        "date = dataset['Date']\n",
        "date.head()\n",
        "len(date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nxcgc_K3ZDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting timestamp and isolating the date\n",
        "dateFormatting = pd.DataFrame({'new_date': pd.date_range('2019-12-31', periods=len(date))})\n",
        "# dateFormatting['new_date'] = [d.date() for d in dateFormatting['my_timestamp']]\n",
        "# dateFormatting['new_time'] = [d.time() for d in dateFormatting['my_timestamp']]\n",
        "print(dateFormatting)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tav3LALn4KN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert date to list so that we can use the dates to plot on graph\n",
        "my_xticks = dateFormatting['new_date'].tolist()\n",
        "print(my_xticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeyD3C8dm1DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['Date']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvb8c8CBIOqM",
        "colab_type": "text"
      },
      "source": [
        "# Plotting Features before Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDGMN3GG6wOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotted prices and dates\n",
        "# Notice how the dates start and end\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import dates as mpl_dates\n",
        "plt.style.use('seaborn')\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = 15,6\n",
        "\n",
        "dates = np.array(dateFormatting['new_date'])\n",
        "price = np.array(dataset['Price'])\n",
        "plt.plot_date(dates,price, linestyle ='solid')\n",
        "plt.gcf().autofmt_xdate()\n",
        "date_format = mpl_dates.DateFormatter('%b,%d,%Y')\n",
        "plt.title('Time Series Price of USD compare to GBP')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.gca().xaxis.set_major_formatter(date_format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqj88upVOPC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(price)\n",
        "test = np.array(dataset['total_deaths'])\n",
        "# print(test)\n",
        "price.shape\n",
        "# test.shape\n",
        "test = np.array(dataset['total_deaths'])\n",
        "list_arr = np.array(test)\n",
        "reversed_arr = list_arr[::-1]\n",
        "reversed_arr.shape\n",
        "price.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T30FJ5ONMRsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adds the accumulative deaths every day of a country\n",
        "# Problem â€“ This needs to be sorted from the end since the way out data is set up - can this be made more efficient?\n",
        "total_us_deaths = dataset['total_deaths']\n",
        "list_arr = np.array(total_us_deaths)\n",
        "reversed_arr = list_arr[::-1]\n",
        "newDay = 0\n",
        "day = []\n",
        "for currentday in reversed_arr:\n",
        "   newDay = currentday + newDay\n",
        "   day.append(newDay)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oivvoebSMpRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plots the accumulative daily death rates of a country\n",
        "dates = np.array(dateFormatting['new_date'])\n",
        "us_deaths = np.array(day)\n",
        "plt.plot_date(dates,us_deaths, linestyle ='solid')\n",
        "plt.gcf().autofmt_xdate()\n",
        "date_format = mpl_dates.DateFormatter('%b,%d,%Y')\n",
        "plt.title('Time Series of Total Deaths of US')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Deaths')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCKLM-XSXt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLvrpnjESZmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWTJypnsv5R3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6NykB3TSJxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price_frame = pd.DataFrame(np.concatenate([price]), columns= [\"Price\"])\n",
        "death_frame = pd.DataFrame(np.concatenate([day]), columns= [\"Total_Deaths_US\"])\n",
        "price_frame.append(death_frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkWFQ8HeIerS",
        "colab_type": "text"
      },
      "source": [
        "# Normalisation of Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUr5EmIHITzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Working Normalised method!!\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "# they are so small that is why\n",
        "normalisation_us_deaths = scaler.fit_transform(death_frame).reshape(-1,1)\n",
        "# loosing precision by converting to decimal but makes more readable\n",
        "# but when plotting it use normalisation_us_deaths\n",
        "normalisation_price = scaler.fit_transform(price_frame).reshape(-1,1)\n",
        "# 8 decimal: readable formate\n",
        "# val = [print(\"{:.8f}\".format(float(x)))for x in normalisation_us_deaths]\n",
        "# print(normalisation_price)\n",
        "# print(val)\n",
        "# print(normalisation_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG6Frxvpw_YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect the data\n",
        "# Look at join distributions of afew pairs of collumns from the training set\n",
        "# Need to revise what this graph does\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Time Series of Total Deaths of US & USD/GBP')\n",
        "plt.plot(normalisation_us_deaths,label=\"us_covid_death_rates\")\n",
        "plt.plot(normalisation_price, label=\"USD/GBP price\")\n",
        "plt.ylabel('Normalisation between 0 and 1')\n",
        "plt.xlabel('Date')\n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrNnj6gGIkmf",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing the Data - Train, Validation & Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJTAF3KWyKwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dataset = np.hstack((normalisation_us_deaths,normalisation_price))\n",
        "new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx35yrWb1MIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test, Validation, Train \n",
        "# This should be ordered in respect to date â€“ not cross validation or random seed\n",
        "# We are going to divide our data based on date because the next data is always dependent on the previous data\n",
        "# 65% of the total length of my datadrame\n",
        "# 35% test size/ validation\n",
        "training_size = int(len(new_dataset)*0.65)\n",
        "test_size = (len(new_dataset)-training_size)//2 # // Rounds down\n",
        "train_data, valset_data, test_data = new_dataset[0:training_size], new_dataset[training_size:(training_size + test_size)],new_dataset[(training_size + test_size):]\n",
        "print(train_data.shape)\n",
        "print(valset_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1CkWzUfVwxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(valset_data[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKERrxeA1sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_size, test_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llongsG7BHW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc0pElKOBO-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lcOjmoBARZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert an array of values into a dataset matrix\n",
        "# Go over what this method does\n",
        "# time_step = how many previous days you have to consider before making a prediction\n",
        "def create_dataset(dataset, time_step=1):\n",
        "  dataX, dataY = [],[]\n",
        "  # then we subtract e.g. timestep=3 : lendataset -3-3 so you are shifting\n",
        "  # iterates from \n",
        "  # -1: guarentees 8th datapoint \n",
        "  # 100 - 1 = 99-1 =5 -> 8th\n",
        "  # i is the starting point of dataset\n",
        "  for i in range(len(dataset)-time_step-1):\n",
        "    a = dataset[i:(i+time_step)]  # i=0, 0,1,2,3\n",
        "    # print(a)\n",
        "    dataX.append(a)\n",
        "    y = dataset[i + time_step][1] \n",
        "    dataY.append(y)\n",
        "  return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjbnyit_VZur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weekly_timestep means taking in 7 days of features at a time \n",
        "# whats the point of writing 7 when it is assigned to 1?\n",
        "weekly_timestep = 7\n",
        "X_train, Y_train = create_dataset(train_data, weekly_timestep)\n",
        "X_val, Y_val = create_dataset(valset_data, weekly_timestep)\n",
        "X_test, Y_test = create_dataset(test_data, weekly_timestep)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bii3I74VfqRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_test.shape), print(Y_test.shape)\n",
        "# Finished preprocessing the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQxS8R3VI2bP",
        "colab_type": "text"
      },
      "source": [
        "# Stacked LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuLjcvVwM7Sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the stacked LSTM model\n",
        "# One LSTM after the other\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "def lstm_model(lr_rate, hidden_layer, optimizer_one):\n",
        "  # we want input_tensor to be a single data point\n",
        "  model = Sequential()\n",
        "  opt = None\n",
        "  if optimizer_one == 'adam':\n",
        "   opt = keras.optimizers.Adam(learning_rate=lr_rate)\n",
        "  else:\n",
        "    opt = keras.optimizers.Adamax(learning_rate=lr_rate)\n",
        "  model.add(InputLayer(input_shape=(7,2)))\n",
        "  model.add(LSTM(hidden_layer,return_sequences=True))\n",
        "  model.add(LSTM(hidden_layer, return_sequences=True))\n",
        "  model.add(LSTM(hidden_layer))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss = 'mean_squared_error', optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkkElpH-cy_h",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "\n",
        "*   Optimizers:\n",
        "[Overview of different Optimizers for neural networks](https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3#:~:text=Adagrad%20eliminates%20the%20need%20to,is%20no%20longer%20able%20learning.)\n",
        "*   [Conceptual Guide for HP Tuning](https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35)\n",
        "*   [Consider this for Grid Search & Random Search](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html)\n",
        "*   [And this](https://stackoverflow.com/questions/58137140/randomizedsearchcv-with-keras-lstm-regression)\n",
        "![Grid Search & Random Search](https://miro.medium.com/max/1200/1*ZTlQm_WRcrNqL-nLnx6GJA.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbgWWCHLcdm",
        "colab_type": "text"
      },
      "source": [
        "## Grid Search\n",
        "\n",
        "*   We must choose candidates for good hidden layers and optimizers. Then plot a comparison.\n",
        "*   List of optimizers: \n",
        "*   optimizers = ['SGD', 'RMSprop','adam','adagrad', 'adadelta', 'adamax','nadam', 'ftrl']\n",
        "*   Callbacks: Restores best weights that is going to give u best model \n",
        "*   Consider changing batch size: slower the batch_size better training, because small dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA11PFS2lemX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_layer_list = [14,20]\n",
        "best_hidden_layer = 1\n",
        "optimizer_list = ['adam','adamax']\n",
        "best_optimizer = 'adam'\n",
        "learning_rate_list = [3e-3, 3e-3]\n",
        "best_learning_rate = 3e-3\n",
        "least_loss = 1\n",
        "\n",
        "\n",
        "\n",
        "for each_hidden_layer in hidden_layer_list:\n",
        "  for each_optimizer in optimizer_list:\n",
        "    for each_learning_rate in learning_rate_list:\n",
        "      our_model = lstm_model(each_learning_rate, each_hidden_layer, each_optimizer)\n",
        "      our_model.fit(X_train, Y_train, \n",
        "                validation_data= \n",
        "                (X_val, Y_val)\n",
        "                , epochs=20,batch_size=10,verbose=1,\n",
        "                callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "      loss = our_model.evaluate(X_test, Y_test) # all results from dictionary\n",
        "      if loss < least_loss:\n",
        "          least_loss = loss\n",
        "          best_learning_rate = each_learning_rate\n",
        "          best_optimizer = each_optimizer\n",
        "          best_hidden_layer = each_hidden_layer\n",
        "  print(least_loss, best_learning_rate, best_optimizer, best_hidden_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne1FGIRhCBx3",
        "colab_type": "text"
      },
      "source": [
        "# Best Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBxAVM4I3NJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_param_model = lstm_model(best_learning_rate, best_hidden_layer, best_optimizer)\n",
        "best_param_model.fit(X_train, Y_train, validation_data= (X_val, Y_val), epochs=20,batch_size=10,verbose=1)\n",
        "loss = best_param_model.evaluate(X_test, Y_test) # all results from dictionary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBny6cUkO4xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction and check performance metric\n",
        "train_predict = best_param_model.predict(X_train)\n",
        "test_predict = best_param_model.predict(X_test)\n",
        "train_predict.shape\n",
        "train_predict.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7ocuWlpPIEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform back to original form\n",
        "train_predict=scaler.inverse_transform(train_predict)\n",
        "test_predict=scaler.inverse_transform(test_predict)\n",
        "# print(train_predict, test_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4ocL2CLJLXg",
        "colab_type": "text"
      },
      "source": [
        "# Performance Metric\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXXILzqPUHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the RMSE performance metrics\n",
        "# Output for the train dataset\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "math.sqrt(mean_squared_error(Y_train, train_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFNm7l86Pt_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use testing\n",
        "math.sqrt(mean_squared_error(Y_test, test_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZc_4QJmZugn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating: How well is model performing \n",
        "# Probability distribution rather than time series\n",
        "# Tensorflow??\n",
        "# KL Divergence\n",
        "# Consider this for evaluation: https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLDivergence\n",
        "# kl divergence measures difference between 2 probability distributions, \n",
        "# measure of what i am predicting and what should have been predicting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nSW23ryI8RU",
        "colab_type": "text"
      },
      "source": [
        "# Actual Data, Train Prediction & Test Prediction data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIaC4DB2UV6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking in the 7 \n",
        "\n",
        "look_back = 7\n",
        "\n",
        "# empty_like: Return a new array with the same shape and type as a given array.\n",
        "# np.nan: Replect all with nan? What does this mean?\n",
        "# What does this do?? 7: predict.length() + 7, :\n",
        "\n",
        "trainPredictPlot = np.empty_like(new_dataset)\n",
        "trainPredictPlot[:,:]= np.nan\n",
        "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
        "print(trainPredictPlot.shape)\n",
        "\n",
        "# empty_like: Return a new array with the same shape and type as a given array.\n",
        "# np.nan: Replect all with nan? What does this mean?\n",
        "# Dont understand what this is doing\n",
        "# What does this do?? predict.length() + (7*2) +2 :(training_size + test_size)] = test_predict\n",
        "\n",
        "testPredictPlot = np.empty_like(new_dataset)\n",
        "testPredictPlot[:,:]= np.nan\n",
        "testPredictPlot[len(train_predict)+(look_back*2)+1: (training_size + test_size)] = test_predict\n",
        "# new_dataset[training_size:(training_size + test_size)]\n",
        "print(testPredictPlot.shape)\n",
        "\n",
        "# Plot baseline and predictions\n",
        "plt.title('Analysis of train, test and actual')\n",
        "plt.plot(scaler.inverse_transform(new_dataset), label=\"actual dataset\")\n",
        "\n",
        "trainPredictPlot = [item[1] for item in trainPredictPlot]â€©\n",
        "plt.plot(trainPredictPlot, label=\"train_predict\")\n",
        "\n",
        "testPredictPlot = [item[1] for item in testPredictPlot]â€©\n",
        "plt.plot(testPredictPlot, label=\"test_predict\")\n",
        "\n",
        "# as you can see th test data and train is divided because we want to test after a specific day\n",
        "plt.ylabel('Forex price & US Death Rates')\n",
        "plt.xlabel('Date')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Printing 2 actual_dataset, train_predict, test_predict because 2 input layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc6YFJvQjBGu",
        "colab_type": "text"
      },
      "source": [
        "# Custom 4 day prediction\n",
        "\n",
        "*   Training train data to predict future days and evaluating the accuracy on validation data.\n",
        "*   Training train data to predict future days, no way of evaluating however this can tell us what is likely to occur.\n",
        "*   Using last 7 days of training data to predict the future custom days(4). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yws-gX7RTB3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking the last 7 days of test\n",
        "# print(test_data[80:])\n",
        "# is this supposed to be test or train_data\n",
        "X_input=test_data[40:].reshape(1,-1)\n",
        "print(X_input)\n",
        "X_input.shape\n",
        "\n",
        "temp_input = train_data[-7:]\n",
        "temp_input\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHAxzKyuTp5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Demonstrate prediction for the next 4 days:\n",
        "\n",
        "# Step 1. 7 days of feature = temp_input taken from the last 7 days of the training set\n",
        "# Step 2  i is the day you are on and we want to predict 4 days: 0,1,2,3 (4 days)\n",
        "# Step 3. Predict the 8th day on each of the 7 days,\n",
        "# Step 4. Add this new predicted day to the list\n",
        "# Step 5. Because we are now taking in the last 7 days + the new predicted value, the list is now size 8  \n",
        "# Step 6. In order to take in 7 days at a time, we shift 1 to the right.\n",
        "# Step 7. We reassign the X_train to the new 7 days\n",
        "# Step 8. We predict the 8th day of the 7 days and continue the process\n",
        "# Step 9. We then add the 8th day to the output list\n",
        "# Step 10. We keep track of the 4 days by incrementing by i at the end-> i = i+1\n",
        "# Step 11. Once we have predicted the 4 days, we return the list of 4 day values.\n",
        "\n",
        "from numpy import array\n",
        "custom_day = 4\n",
        "lst_output=[]\n",
        "n_steps=7\n",
        "i=0\n",
        "X_input = temp_input\n",
        "# print(X_input)\n",
        "while(i<custom_day):\n",
        "  if(len(temp_input)>7):\n",
        "    # print(temp_input)  \n",
        "    X_input=array(temp_input[1:])\n",
        "    # print(\"{} day input {}\".format(i,X_input))\n",
        "    # Correct input shape (1,7,2)\n",
        "    X_input=X_input.reshape((1,n_steps,2))\n",
        "    # Prediction of X_input\n",
        "    print(X_input)\n",
        "    Y_hat = best_param_model.predict(X_input)\n",
        "    # print(Y_hat)\n",
        "    temp_input = np.append(temp_input, Y_hat)\n",
        "    temp_input=temp_input[1:]\n",
        "    lst_output.append(Y_hat[0][0])\n",
        "    i=i+1\n",
        "  else:\n",
        "    X_input=X_input.reshape((1,n_steps,2))\n",
        "    print(X_input)\n",
        "    Y_hat = best_param_model.predict(X_input, verbose=0)\n",
        "    # print(Y_hat)\n",
        "    temp_input = np.append(temp_input, Y_hat)\n",
        "    lst_output.append(Y_hat[0][0])\n",
        "    i=i+1\n",
        "# [0.039997526, 0.035484765, 0.029113937, 0.025276443]\n",
        "# Print the predicted forex price for the next 4 days \n",
        "print(lst_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0DCdCn2jwxO",
        "colab_type": "text"
      },
      "source": [
        "### Plotting evaluation on graph. Training output versus first 4 days of the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_5opaqHfcZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom day plotted\n",
        "day_new = custom_day-1\n",
        "\n",
        "# Predicted 4 days from training\n",
        "# Plot and label\n",
        "# Transform back to original value before normalised\n",
        "custom_day_prediction = np.array(lst_output)\n",
        "print(custom_day_prediction)\n",
        "plt.plot(custom_day_prediction, label=\"Predicted\")\n",
        "# plt.plot(day_new, scaler.inverse_transform([custom_day_prediction]))\n",
        "\n",
        "# Actual 4 days from validation\n",
        "# Plot and label\n",
        "# Transform back to original value before normalised\n",
        "val_data = [item[1] for item in valset_data] \n",
        "custom_day_validation = val_data[:custom_day]\n",
        "print(custom_day_validation)\n",
        "plt.plot(custom_day_validation, label=\"Actual\")\n",
        "# plt.plot(day_new, scaler.inverse_transform([custom_day_validation]))\n",
        "\n",
        "# Information\n",
        "plt.title('Predicted custom day from training data versus actual custom day from validation data')\n",
        "plt.xlabel('Number of custom days')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# our_model = [0.7160597  0.46945733 0.75719374 0.4917585 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X1xZG9bWNx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect what we got as a result to the future\n",
        "day_new = custom_day-1\n",
        "\n",
        "# Predicted 4 days from training\n",
        "# Plot and label\n",
        "# Transform back to original value before normalised\n",
        "custom_day_prediction = np.array(lst_output)\n",
        "print(custom_day_prediction)\n",
        "plt.plot(custom_day_prediction, label=\"Predicted\")\n",
        "plt.plot(day_new, scaler.inverse_transform([custom_day_prediction]))\n",
        "\n",
        "# Actual 4 days from validation\n",
        "# Plot and label\n",
        "# Transform back to original value before normalised\n",
        "# Start on 4 instead\n",
        "val_data = [item[1] for item in valset_data] \n",
        "custom_day_validation = val_data[custom_day:]\n",
        "plt.plot(custom_day_validation, label=\"Actual\")\n",
        "plt.plot(day_new, scaler.inverse_transform([custom_day_validation]))\n",
        "\n",
        "# Information\n",
        "plt.title('Predicted custom day from training data versus actual custom day from validation data')\n",
        "plt.xlabel('Number of custom days')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-JX2Sw3-2k4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom day plotted\n",
        "day_new = custom_day-1\n",
        "\n",
        "# Predicted 4 days from training\n",
        "# Plot and label\n",
        "# Transform back to original value before normalised\n",
        "custom_day_prediction = np.array(lst_output)\n",
        "print(custom_day_prediction)\n",
        "plt.plot(custom_day_prediction, label=\"Predicted\")\n",
        "# plt.plot(day_new, scaler.inverse_transform([custom_day_prediction]))\n",
        "\n",
        "# Actual 4 days from validation\n",
        "# Plot and label\n",
        "# Transform back to original value before normalised\n",
        "val_data = [item[1] for item in valset_data] \n",
        "custom_day_validation = val_data[:custom_day]\n",
        "print(custom_day_validation)\n",
        "plt.plot(custom_day_validation, label=\"Actual\")\n",
        "# plt.plot(day_new, scaler.inverse_transform([custom_day_validation]))\n",
        "\n",
        "# Information\n",
        "plt.title('Predicted custom day from training data versus actual custom day from validation data')\n",
        "plt.xlabel('Number of custom days')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeOGd1UPV8jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What we have:\n",
        "# day_new=np.arange(1,8)\n",
        "# print(day_new)\n",
        "# What we want to predict:\n",
        "# day_pred=np.arange(8,12)\n",
        "# print(day_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o0GfssHMvIL",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion and next steps\n",
        "We explored the LSTM algorithms:\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "In conclusion it seems that .."
      ]
    }
  ]
}